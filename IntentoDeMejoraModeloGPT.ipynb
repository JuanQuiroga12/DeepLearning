{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPryTmXGxZQ8269hgbMl5GE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanQuiroga12/DeepLearning/blob/main/IntentoDeMejoraModeloGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c9HNTobiaZzN",
        "outputId": "78f6aeed-80ad-46cb-b9c8-ae633805517e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.52.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.31.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.28.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install peft accelerate bitsandbytes rouge-score nltk wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLTe59cadIkQ",
        "outputId": "97670f21-e682-4c91-8f38-98859f083ffc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "t9oqarnZfs5X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4mtFUJ1gCzs",
        "outputId": "6bb881d2-2354-45d8-eb30-ed1c3e1a624e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.52.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Generador Automático de Preguntas con GPT-Neo - VERSIÓN OPTIMIZADA\n",
        "# Universidad Militar Nueva Granada\n",
        "# Deep Learning - Homework 7 - Versión Mejorada\n",
        "\n",
        "Este notebook implementa una versión significativamente mejorada del generador de preguntas,\n",
        "con técnicas avanzadas de fine-tuning, hiperparámetros optimizados y arquitectura robusta\n",
        "para generar preguntas de alta calidad y diversidad.\n",
        "Mejoras implementadas:\n",
        "- Solución del problema de NaN loss\n",
        "- LoRA fine-tuning para eficiencia\n",
        "- Curriculum learning\n",
        "- Data augmentation avanzada\n",
        "- Hiperparámetros optimizados\n",
        "- Evaluación robusta con métricas específicas\n",
        "- Patrones de preguntas más sofisticados\n",
        "\n",
        "Trabajo de: Juan Quiroga y Marielby Paz\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import textwrap\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    get_scheduler,\n",
        "    AutoConfig,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "import datasets\n",
        "from datasets import Dataset as HFDataset, DatasetDict\n",
        "import wandb\n",
        "wandb.login()\n",
        "import gc\n",
        "import time\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType\n",
        ")\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzxqdYh7a-hJ",
        "outputId": "8abefaad-9d4a-414b-a62a-396806bc42bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mest-juand-quiroga\u001b[0m (\u001b[33mest-juand-quiroga-universidad-militar-nueva-granada\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar recursos de NLTK necesarios\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURACIÓN AVANZADA DE HIPERPARÁMETROS\n",
        "# ============================================================================\n",
        "\n",
        "# Configuración de reproducibilidad\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Configuración del dispositivo y memoria\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Utilizando device: {DEVICE}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memoria GPU total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Modelo base (más grande para mejor calidad)\n",
        "MODEL_NAME = \"EleutherAI/gpt-neo-1.3B\"\n",
        "BACKUP_MODEL = \"EleutherAI/gpt-neo-125M\"\n",
        "\n",
        "# Hiperparámetros de entrenamiento optimizados\n",
        "TRAINING_CONFIG = {\n",
        "    \"learning_rate\": 1e-4,           # Reducido para estabilidad\n",
        "    \"weight_decay\": 0.01,            # Regularización\n",
        "    \"warmup_ratio\": 0.1,             # 10% de warmup\n",
        "    \"num_epochs\": 12,                # Más épocas para mejor aprendizaje\n",
        "    \"batch_size\": 2,                 # Aumentado ligeramente\n",
        "    \"gradient_accumulation_steps\": 16, # Mayor acumulación\n",
        "    \"max_grad_norm\": 1.0,            # Gradient clipping\n",
        "    \"lr_scheduler_type\": \"cosine\",   # Cosine annealing\n",
        "    \"dataloader_num_workers\": 4,     # Paralelización\n",
        "    \"fp16\": True,                    # Precisión mixta\n",
        "    \"save_strategy\": \"epoch\",\n",
        "    \"eval_strategy\": \"epoch\",\n",
        "    \"logging_steps\": 50,\n",
        "    \"save_total_limit\": 3,\n",
        "    \"load_best_model_at_end\": True,\n",
        "    \"metric_for_best_model\": \"eval_loss\",\n",
        "    \"greater_is_better\": False\n",
        "}\n",
        "\n",
        "# Configuración de LoRA para fine-tuning eficiente\n",
        "LORA_CONFIG = {\n",
        "    \"r\": 16,                        # Rank de LoRA (aumentado)\n",
        "    \"lora_alpha\": 32,               # Alpha scaling\n",
        "    \"lora_dropout\": 0.1,            # Dropout para regularización\n",
        "    \"bias\": \"none\",                 # No bias en LoRA\n",
        "    \"task_type\": TaskType.CAUSAL_LM,\n",
        "    \"target_modules\": [\"c_attn\", \"c_proj\", \"c_fc\"]  # Módulos específicos de GPT-Neo\n",
        "}\n",
        "\n",
        "# Configuración de generación optimizada\n",
        "GENERATION_CONFIG = {\n",
        "    \"max_new_tokens\": 80,\n",
        "    \"temperature\": 0.8,\n",
        "    \"top_p\": 0.9,\n",
        "    \"top_k\": 50,\n",
        "    \"num_beams\": 5,\n",
        "    \"do_sample\": True,\n",
        "    \"early_stopping\": True,\n",
        "    \"no_repeat_ngram_size\": 3,\n",
        "    \"repetition_penalty\": 1.2\n",
        "}\n",
        "\n",
        "# Configuración de paths\n",
        "OUTPUT_DIR = \"advanced_question_generator\"\n",
        "MODEL_SAVE_DIR = os.path.join(OUTPUT_DIR, \"best_model\")\n",
        "LOGS_DIR = os.path.join(OUTPUT_DIR, \"logs\")\n",
        "\n",
        "# Crear directorios\n",
        "for dir_path in [OUTPUT_DIR, MODEL_SAVE_DIR, LOGS_DIR]:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET AVANZADO CON MAYOR DIVERSIDAD Y COMPLEJIDAD\n",
        "# ============================================================================\n",
        "\n",
        "class AdvancedQuestionDataset(Dataset):\n",
        "    \"\"\"Dataset avanzado con patrones de preguntas más sofisticados y diversos\"\"\"\n",
        "\n",
        "    def __init__(self, file_path=\"/content/xquad.es.json\", use_synthetic=False, tokenizer=None, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.examples = []\n",
        "\n",
        "        # Intentar cargar XQuAD primero\n",
        "        if file_path and os.path.exists(file_path) and not use_synthetic:\n",
        "            self._load_xquad(file_path)\n",
        "        else:\n",
        "            use_synthetic = True\n",
        "\n",
        "        if use_synthetic or len(self.examples) < 100:\n",
        "            print(\"Generando dataset sintético avanzado...\")\n",
        "            synthetic_examples = self._create_advanced_synthetic_dataset()\n",
        "            self.examples.extend(synthetic_examples)\n",
        "            print(f\"Total de ejemplos: {len(self.examples)}\")\n",
        "\n",
        "    def _load_xquad(self, file_path):\n",
        "        \"\"\"Carga el dataset XQuAD con procesamiento mejorado\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            for article in data['data']:\n",
        "                for paragraph in article['paragraphs']:\n",
        "                    context = paragraph['context']\n",
        "                    for qa in paragraph['qas']:\n",
        "                        question = qa['question']\n",
        "                        answers = qa['answers']\n",
        "                        if answers and len(answers) > 0:\n",
        "                            answer_text = answers[0]['text']\n",
        "\n",
        "                            # Añadir ejemplo original\n",
        "                            self.examples.append({\n",
        "                                'context': context,\n",
        "                                'question': question,\n",
        "                                'answer': answer_text,\n",
        "                                'question_type': self._classify_question_type(question)\n",
        "                            })\n",
        "\n",
        "                            # Data augmentation: crear variaciones\n",
        "                            variations = self._create_question_variations(context, question, answer_text)\n",
        "                            self.examples.extend(variations)\n",
        "\n",
        "            print(f\"Cargados {len(self.examples)} ejemplos de XQuAD (incluyendo aumentación)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error al cargar XQuAD: {e}\")\n",
        "\n",
        "    def _classify_question_type(self, question):\n",
        "        \"\"\"Clasifica el tipo de pregunta para crear mejor diversidad\"\"\"\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        if any(word in question_lower for word in ['qué', 'cuál', 'cuáles']):\n",
        "            return 'what'\n",
        "        elif any(word in question_lower for word in ['quién', 'quiénes']):\n",
        "            return 'who'\n",
        "        elif any(word in question_lower for word in ['cuándo']):\n",
        "            return 'when'\n",
        "        elif any(word in question_lower for word in ['dónde']):\n",
        "            return 'where'\n",
        "        elif any(word in question_lower for word in ['por qué', 'cómo']):\n",
        "            return 'why_how'\n",
        "        elif any(word in question_lower for word in ['cuánto', 'cuántos', 'cuántas']):\n",
        "            return 'quantity'\n",
        "        else:\n",
        "            return 'general'\n",
        "\n",
        "    def _create_question_variations(self, context, original_question, answer):\n",
        "        \"\"\"Crea variaciones de una pregunta para aumentar diversidad\"\"\"\n",
        "        variations = []\n",
        "        question_type = self._classify_question_type(original_question)\n",
        "\n",
        "        # Patrones alternativos según el tipo de pregunta\n",
        "        if question_type == 'what':\n",
        "            alt_patterns = [\n",
        "                f\"¿Cuál es la definición de {answer}?\",\n",
        "                f\"¿A qué se refiere el término {answer}?\",\n",
        "                f\"¿Qué significa {answer} en este contexto?\"\n",
        "            ]\n",
        "        elif question_type == 'who':\n",
        "            alt_patterns = [\n",
        "                f\"¿Quién fue {answer}?\",\n",
        "                f\"¿Cuál es la identidad de {answer}?\",\n",
        "                f\"¿A quién se refiere cuando menciona {answer}?\"\n",
        "            ]\n",
        "        elif question_type == 'when':\n",
        "            alt_patterns = [\n",
        "                f\"¿En qué momento ocurrió {answer}?\",\n",
        "                f\"¿Cuál fue la fecha de {answer}?\",\n",
        "                f\"¿Cuándo tuvo lugar {answer}?\"\n",
        "            ]\n",
        "        else:\n",
        "            alt_patterns = [\n",
        "                f\"¿Cuál es la importancia de {answer}?\",\n",
        "                f\"¿Por qué es relevante {answer}?\",\n",
        "                f\"¿Qué papel juega {answer}?\"\n",
        "            ]\n",
        "\n",
        "        # Crear máximo 2 variaciones por pregunta original\n",
        "        for i, pattern in enumerate(alt_patterns[:2]):\n",
        "            variations.append({\n",
        "                'context': context,\n",
        "                'question': pattern,\n",
        "                'answer': answer,\n",
        "                'question_type': question_type\n",
        "            })\n",
        "\n",
        "        return variations\n",
        "\n",
        "    def _create_advanced_synthetic_dataset(self):\n",
        "        \"\"\"Crea un dataset sintético mucho más diverso y complejo\"\"\"\n",
        "\n",
        "        # Contextos educativos más complejos y diversos\n",
        "        advanced_contexts = [\n",
        "            {\n",
        "                \"text\": \"La teoría cuántica revolucionó nuestra comprensión de la física a nivel subatómico. Max Planck introdujo el concepto de cuantización de la energía en 1900, estableciendo que la energía se emite y absorbe en paquetes discretos llamados cuantos. Posteriormente, Albert Einstein explicó el efecto fotoeléctrico utilizando esta teoría, por lo cual recibió el Premio Nobel de Física en 1921. Werner Heisenberg desarrolló el principio de incertidumbre, que establece que no se puede conocer simultáneamente con precisión absoluta la posición y el momento de una partícula. Erwin Schrödinger formuló la famosa ecuación que describe la evolución temporal de los sistemas cuánticos, y su experimento mental del gato ilustra las paradojas de la mecánica cuántica cuando se aplica a objetos macroscópicos.\",\n",
        "                \"domain\": \"física\",\n",
        "                \"complexity\": \"avanzado\"\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"El proceso de fotosíntesis es fundamental para la vida en la Tierra y ocurre en dos fases principales. La fase lumínica, que tiene lugar en los tilacoides de los cloroplastos, utiliza la energía solar para dividir moléculas de agua, liberando oxígeno como subproducto y generando ATP y NADPH. La clorofila a y b, junto con otros pigmentos accesorios como los carotenoides, capturan diferentes longitudes de onda de la luz. La fase oscura o ciclo de Calvin ocurre en el estroma del cloroplasto, donde el CO₂ atmosférico se fija y reduce utilizando el ATP y NADPH producidos en la fase lumínica. La enzima RuBisCo cataliza la reacción de carboxilación inicial, incorporando CO₂ a la ribulosa-1,5-bifosfato. Este proceso genera glucosa, que sirve como fuente de energía para la planta y base de las cadenas alimentarias terrestres.\",\n",
        "                \"domain\": \"biología\",\n",
        "                \"complexity\": \"intermedio\"\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"La Revolución Industrial transformó radicalmente la sociedad europea entre 1760 y 1840, marcando la transición de una economía agrícola y artesanal a una industrial y mecanizada. James Watt perfeccionó la máquina de vapor en 1769, lo que permitió su aplicación masiva en fábricas y transporte. La invención del telar mecánico por Edmund Cartwright en 1785 revolucionó la industria textil, mientras que la locomotora de vapor de George Stephenson en 1814 transformó el transporte. Estos avances tecnológicos causaron profundos cambios sociales: el surgimiento de la clase obrera industrial, la migración masiva del campo a las ciudades, y nuevas formas de organización laboral. Adam Smith desarrolló las teorías económicas del libre mercado que justificaron ideológicamente estos cambios, mientras que pensadores como Karl Marx analizaron críticamente las consecuencias sociales de la industrialización.\",\n",
        "                \"domain\": \"historia\",\n",
        "                \"complexity\": \"intermedio\"\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Los ecosistemas marinos presentan una compleja red de interacciones entre organismos y su ambiente físico-químico. La zona eufótica, donde penetra suficiente luz solar para la fotosíntesis, alberga al fitoplancton, base de la cadena trófica marina. Estas microalgas unicelulares, incluyendo diatomeas y dinoflagelados, realizan aproximadamente el 50% de la fotosíntesis global. El zooplancton, compuesto por organismos como copépodos y krill, se alimenta del fitoplancton y a su vez sirve de alimento para peces pequeños. Los niveles tróficos superiores incluyen peces carnívoros, cefalópodos y mamíferos marinos. La bomba biológica transporta carbono desde la superficie hasta las profundidades oceánicas cuando los organismos mueren y se hunden, contribuyendo significativamente al ciclo global del carbono. Los arrecifes de coral, considerados las 'selvas tropicales del mar', albergan una biodiversidad excepcional debido a la simbiosis entre pólipos coralinos y zooxantelas.\",\n",
        "                \"domain\": \"biología_marina\",\n",
        "                \"complexity\": \"avanzado\"\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"La inteligencia artificial ha evolucionado desde simples sistemas basados en reglas hasta redes neuronales profundas capaces de tareas complejas. El aprendizaje automático (machine learning) permite a las máquinas mejorar su rendimiento mediante la experiencia, sin programación explícita para cada tarea. Las redes neuronales artificiales, inspiradas en el funcionamiento del cerebro humano, utilizan capas de neuronas interconectadas para procesar información. El aprendizaje profundo (deep learning) emplea arquitecturas con múltiples capas ocultas, permitiendo el reconocimiento de patrones complejos en datos de alta dimensionalidad. Las redes neuronales convolucionales (CNN) han revolucionado el procesamiento de imágenes, mientras que las redes recurrentes (RNN) y los transformers han avanzado significativamente el procesamiento de lenguaje natural. Los modelos generativos como GPT y DALL-E demuestran capacidades creativas antes consideradas exclusivamente humanas.\",\n",
        "                \"domain\": \"tecnología\",\n",
        "                \"complexity\": \"avanzado\"\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"El sistema inmunológico humano constituye una defensa compleja contra patógenos y sustancias extrañas. La inmunidad innata proporciona la primera línea de defensa mediante barreras físicas como la piel y mucosas, células fagocíticas como neutrófilos y macrófagos, y el sistema del complemento. La inmunidad adaptativa, más específica y con memoria, involucra linfocitos T y B. Los linfocitos T helper coordinan la respuesta inmune, los T citotóxicos destruyen células infectadas, y los linfocitos B producen anticuerpos específicos contra antígenos particulares. La presentación de antígenos por células dendríticas y macrófagos es crucial para activar la respuesta adaptativa. Los órganos linfoides primarios (médula ósea y timo) generan y maduran las células inmunes, mientras que los secundarios (ganglios linfáticos, bazo) son sitios de activación. Las vacunas aprovechan la memoria inmunológica para prevenir enfermedades infecciosas.\",\n",
        "                \"domain\": \"inmunología\",\n",
        "                \"complexity\": \"avanzado\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Patrones de preguntas más sofisticados y variados\n",
        "        advanced_question_patterns = {\n",
        "            \"conceptual\": [\n",
        "                \"¿Qué concepto fundamental explica {}?\",\n",
        "                \"¿Cuál es el principio básico detrás de {}?\",\n",
        "                \"¿Cómo se define {} en este contexto?\",\n",
        "                \"¿Qué teoría sustenta el fenómeno de {}?\",\n",
        "                \"¿Cuál es la base científica de {}?\"\n",
        "            ],\n",
        "            \"causal\": [\n",
        "                \"¿Qué factores causaron {}?\",\n",
        "                \"¿Por qué ocurre el proceso de {}?\",\n",
        "                \"¿Cuáles son las razones detrás de {}?\",\n",
        "                \"¿Qué mecanismo explica {}?\",\n",
        "                \"¿Cómo se origina el fenómeno de {}?\"\n",
        "            ],\n",
        "            \"comparative\": [\n",
        "                \"¿En qué se diferencia {} de otros conceptos similares?\",\n",
        "                \"¿Cuáles son las ventajas de {} sobre alternativas?\",\n",
        "                \"¿Qué características distinguen a {} de otros elementos?\",\n",
        "                \"¿Cómo se compara {} con procesos relacionados?\",\n",
        "                \"¿Qué hace único a {} en su categoría?\"\n",
        "            ],\n",
        "            \"functional\": [\n",
        "                \"¿Cuál es la función principal de {}?\",\n",
        "                \"¿Qué papel desempeña {} en el sistema?\",\n",
        "                \"¿Cómo contribuye {} al proceso general?\",\n",
        "                \"¿Para qué sirve {} en este contexto?\",\n",
        "                \"¿Qué importancia tiene {} en el funcionamiento?\"\n",
        "            ],\n",
        "            \"temporal\": [\n",
        "                \"¿Cuándo se desarrolló {}?\",\n",
        "                \"¿En qué período histórico ocurrió {}?\",\n",
        "                \"¿Qué año marca el inicio de {}?\",\n",
        "                \"¿Cuál fue la cronología de {}?\",\n",
        "                \"¿En qué momento se estableció {}?\"\n",
        "            ],\n",
        "            \"analytical\": [\n",
        "                \"¿Qué implicaciones tiene {} para la sociedad?\",\n",
        "                \"¿Cuáles son las consecuencias de {}?\",\n",
        "                \"¿Qué efectos produce {} en el sistema?\",\n",
        "                \"¿Cómo impacta {} en otros procesos?\",\n",
        "                \"¿Qué cambios genera {} en el entorno?\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        examples = []\n",
        "\n",
        "        # Generar ejemplos para cada contexto\n",
        "        for context_info in advanced_contexts:\n",
        "            context = context_info[\"text\"]\n",
        "            domain = context_info[\"domain\"]\n",
        "            complexity = context_info[\"complexity\"]\n",
        "\n",
        "            # Extraer entidades y conceptos clave del contexto\n",
        "            key_entities = self._extract_key_entities(context)\n",
        "\n",
        "            # Generar múltiples preguntas por contexto (12-15 por contexto)\n",
        "            questions_per_context = 14 if complexity == \"avanzado\" else 12\n",
        "\n",
        "            for _ in range(questions_per_context):\n",
        "                # Seleccionar tipo de pregunta y entidad aleatoriamente\n",
        "                question_type = random.choice(list(advanced_question_patterns.keys()))\n",
        "                pattern = random.choice(advanced_question_patterns[question_type])\n",
        "                entity = random.choice(key_entities)\n",
        "\n",
        "                # Generar pregunta\n",
        "                question = pattern.format(entity)\n",
        "\n",
        "                # Generar respuesta más sofisticada\n",
        "                answer = self._generate_sophisticated_answer(context, entity, question_type)\n",
        "\n",
        "                examples.append({\n",
        "                    'context': context,\n",
        "                    'question': question,\n",
        "                    'answer': answer,\n",
        "                    'question_type': question_type,\n",
        "                    'domain': domain,\n",
        "                    'complexity': complexity\n",
        "                })\n",
        "\n",
        "        # Multiplicar ejemplos para tener un dataset más grande (aproximadamente 500-600 ejemplos)\n",
        "        examples = examples * 6\n",
        "\n",
        "        # Añadir ruido controlado para mayor variabilidad\n",
        "        examples.extend(self._add_controlled_variations(examples[:100]))\n",
        "\n",
        "        return examples\n",
        "\n",
        "    def _extract_key_entities(self, text):\n",
        "        \"\"\"Extrae entidades y conceptos clave del texto de forma más inteligente\"\"\"\n",
        "        # Patrones para identificar conceptos importantes\n",
        "        patterns = [\n",
        "            r'\\b[A-Z][a-z]+ [A-Z][a-z]+\\b',  # Nombres propios de dos palabras\n",
        "            r'\\b[A-Z][a-zA-ZáéíóúñÁÉÍÓÚÑ]{4,}\\b',  # Palabras importantes con mayúscula\n",
        "            r'\\b(?:principio|teoría|ley|efecto|proceso|método|sistema|concepto) de [A-Za-záéíóúñÁÉÍÓÚÑ\\s]+\\b',  # Conceptos específicos\n",
        "            r'\\b\\d{4}\\b',  # Años\n",
        "            r'\\b[a-záéíóúñ]+(?:ción|sión|miento|ismo|idad)\\b'  # Conceptos abstractos\n",
        "        ]\n",
        "\n",
        "        entities = []\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            entities.extend(matches)\n",
        "\n",
        "        # Limpiar y filtrar entidades\n",
        "        entities = [entity.strip() for entity in entities if len(entity.strip()) > 3]\n",
        "        entities = list(set(entities))  # Eliminar duplicados\n",
        "\n",
        "        # Si no encontramos suficientes entidades, extraer términos técnicos\n",
        "        if len(entities) < 5:\n",
        "            words = re.findall(r'\\b[a-záéíóúñA-ZÁÉÍÓÚÑ]{6,}\\b', text)\n",
        "            entities.extend(words[:10])\n",
        "\n",
        "        return entities[:15]  # Limitar a 15 entidades por contexto\n",
        "\n",
        "    def _generate_sophisticated_answer(self, context, entity, question_type):\n",
        "        \"\"\"Genera respuestas más sofisticadas basadas en el contexto y tipo de pregunta\"\"\"\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', context)\n",
        "        relevant_sentences = [s for s in sentences if entity.lower() in s.lower()]\n",
        "\n",
        "        if not relevant_sentences:\n",
        "            relevant_sentences = sentences[:2]  # Tomar las primeras dos oraciones\n",
        "\n",
        "        # Generar respuesta según el tipo de pregunta\n",
        "        if question_type == \"conceptual\":\n",
        "            # Buscar definiciones o explicaciones\n",
        "            for sentence in relevant_sentences:\n",
        "                if any(word in sentence.lower() for word in ['es', 'son', 'define', 'significa']):\n",
        "                    # Extraer la parte definitoria\n",
        "                    parts = sentence.split(',')\n",
        "                    return parts[0].strip().replace(entity, '').strip()\n",
        "\n",
        "        elif question_type == \"temporal\":\n",
        "            # Buscar fechas o períodos\n",
        "            dates = re.findall(r'\\b\\d{4}\\b|\\b(?:siglo|año) \\w+', context)\n",
        "            if dates:\n",
        "                return dates[0]\n",
        "\n",
        "        # Respuesta por defecto: extraer fragmento relevante\n",
        "        if relevant_sentences:\n",
        "            sentence = relevant_sentences[0]\n",
        "            words = sentence.split()\n",
        "            if len(words) > 10:\n",
        "                # Tomar una porción central de la oración\n",
        "                start = len(words) // 4\n",
        "                end = start + min(8, len(words) // 2)\n",
        "                return ' '.join(words[start:end])\n",
        "            else:\n",
        "                return sentence\n",
        "\n",
        "        return entity  # Fallback\n",
        "\n",
        "    def _add_controlled_variations(self, base_examples):\n",
        "        \"\"\"Añade variaciones controladas para aumentar la diversidad\"\"\"\n",
        "        variations = []\n",
        "\n",
        "        variation_patterns = [\n",
        "            (\"¿Qué\", \"¿Cuál\"),\n",
        "            (\"¿Cuál es\", \"¿En qué consiste\"),\n",
        "            (\"¿Por qué\", \"¿Cuál es la razón por la que\"),\n",
        "            (\"¿Cómo\", \"¿De qué manera\"),\n",
        "            (\"proceso\", \"mecanismo\"),\n",
        "            (\"sistema\", \"estructura\"),\n",
        "            (\"importante\", \"fundamental\"),\n",
        "            (\"principal\", \"primordial\")\n",
        "        ]\n",
        "\n",
        "        for example in base_examples[:50]:  # Solo 50 variaciones adicionales\n",
        "            new_example = example.copy()\n",
        "\n",
        "            # Aplicar una variación aleatoria\n",
        "            if random.random() < 0.7:  # 70% de probabilidad\n",
        "                original_question = example['question']\n",
        "                for old, new in variation_patterns:\n",
        "                    if old in original_question:\n",
        "                        new_example['question'] = original_question.replace(old, new, 1)\n",
        "                        break\n",
        "\n",
        "                variations.append(new_example)\n",
        "\n",
        "        return variations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "\n",
        "        # Formato mejorado para el entrenamiento\n",
        "        context = example['context']\n",
        "        question = example['question']\n",
        "        answer = example['answer']\n",
        "\n",
        "        # Acortar contexto si es muy largo\n",
        "        max_context_length = self.max_length // 2\n",
        "        if len(context) > max_context_length:\n",
        "            # Intentar cortar en una oración completa\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', context)\n",
        "            truncated_context = \"\"\n",
        "            for sentence in sentences:\n",
        "                if len(truncated_context + sentence) < max_context_length:\n",
        "                    truncated_context += sentence + \" \"\n",
        "                else:\n",
        "                    break\n",
        "            context = truncated_context.strip() or context[:max_context_length]\n",
        "\n",
        "        # Formato mejorado con instrucciones más claras\n",
        "        instruction = \"Genera una pregunta relevante basada en el contexto y la respuesta proporcionada.\"\n",
        "        full_text = f\"### Instrucción: {instruction}\\n### Contexto: {context}\\n### Respuesta: {answer}\\n### Pregunta: {question}\"\n",
        "\n",
        "        # Tokenización con manejo de errores\n",
        "        try:\n",
        "            encoding = self.tokenizer(\n",
        "                full_text,\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            input_ids = encoding[\"input_ids\"].squeeze()\n",
        "            attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "            # Labels para entrenamiento causal\n",
        "            labels = input_ids.clone()\n",
        "\n",
        "            # Enmascarar la parte de instrucción y contexto\n",
        "            instruction_text = f\"### Instrucción: {instruction}\\n### Contexto: {context}\\n### Respuesta: {answer}\\n### Pregunta:\"\n",
        "            try:\n",
        "                instruction_tokens = self.tokenizer.encode(instruction_text, add_special_tokens=False)\n",
        "                instruction_length = min(len(instruction_tokens), len(labels))\n",
        "                labels[:instruction_length] = -100\n",
        "            except:\n",
        "                # Si hay error, enmascarar los primeros 3/4 del texto\n",
        "                mask_length = len(labels) * 3 // 4\n",
        "                labels[:mask_length] = -100\n",
        "\n",
        "            return {\n",
        "                \"input_ids\": input_ids,\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"labels\": labels\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error al tokenizar ejemplo {idx}: {e}\")\n",
        "            # Retornar tensores vacíos en caso de error\n",
        "            return {\n",
        "                \"input_ids\": torch.zeros(self.max_length, dtype=torch.long),\n",
        "                \"attention_mask\": torch.zeros(self.max_length, dtype=torch.long),\n",
        "                \"labels\": torch.full((self.max_length,), -100, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "# ============================================================================\n",
        "# FUNCIONES DE CARGA Y CONFIGURACIÓN DEL MODELO\n",
        "# ============================================================================\n",
        "\n",
        "def load_model_with_lora(model_name, device):\n",
        "    \"\"\"Carga el modelo con configuración LoRA para fine-tuning eficiente\"\"\"\n",
        "    print(f\"Cargando modelo {model_name} con configuración LoRA...\")\n",
        "\n",
        "    try:\n",
        "        # Configuración de cuantización para ahorrar memoria\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        # Cargar modelo base\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        # Preparar modelo para entrenamiento con LoRA\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "        # Configurar LoRA\n",
        "        lora_config = LoraConfig(**LORA_CONFIG)\n",
        "        model = get_peft_model(model, lora_config)\n",
        "\n",
        "        # Mostrar parámetros entrenables\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error con cuantización, intentando sin ella: {e}\")\n",
        "\n",
        "        # Cargar sin cuantización como fallback\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "        )\n",
        "\n",
        "        # Aplicar LoRA sin cuantización\n",
        "        lora_config = LoraConfig(**LORA_CONFIG)\n",
        "        model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Cargar tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Configurar pad token\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# ============================================================================\n",
        "# MÉTRICAS DE EVALUACIÓN AVANZADAS\n",
        "# ============================================================================\n",
        "\n",
        "class QuestionGenerationMetrics:\n",
        "    \"\"\"Clase para calcular métricas específicas de generación de preguntas\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        self.smoothing_function = SmoothingFunction().method1\n",
        "\n",
        "    def calculate_bleu(self, reference, hypothesis):\n",
        "        \"\"\"Calcula BLEU score entre referencia e hipótesis\"\"\"\n",
        "        try:\n",
        "            reference_tokens = reference.lower().split()\n",
        "            hypothesis_tokens = hypothesis.lower().split()\n",
        "\n",
        "            if len(hypothesis_tokens) == 0:\n",
        "                return 0.0\n",
        "\n",
        "            return sentence_bleu(\n",
        "                [reference_tokens],\n",
        "                hypothesis_tokens,\n",
        "                smoothing_function=self.smoothing_function\n",
        "            )\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_rouge(self, reference, hypothesis):\n",
        "        \"\"\"Calcula ROUGE scores\"\"\"\n",
        "        try:\n",
        "            scores = self.rouge_scorer.score(reference, hypothesis)\n",
        "            return {\n",
        "                'rouge1': scores['rouge1'].fmeasure,\n",
        "                'rouge2': scores['rouge2'].fmeasure,\n",
        "                'rougeL': scores['rougeL'].fmeasure\n",
        "            }\n",
        "        except:\n",
        "            return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
        "\n",
        "    def calculate_diversity_metrics(self, questions):\n",
        "        \"\"\"Calcula métricas de diversidad en las preguntas generadas\"\"\"\n",
        "        if not questions:\n",
        "            return {'unique_ratio': 0.0, 'avg_length': 0.0, 'type_diversity': 0.0}\n",
        "\n",
        "        # Ratio de preguntas únicas\n",
        "        unique_questions = set(questions)\n",
        "        unique_ratio = len(unique_questions) / len(questions)\n",
        "\n",
        "        # Longitud promedio\n",
        "        avg_length = np.mean([len(q.split()) for q in questions])\n",
        "\n",
        "        # Diversidad de tipos de pregunta\n",
        "        question_starters = [q.split()[0].lower() if q.split() else '' for q in questions]\n",
        "        unique_starters = set(question_starters)\n",
        "        type_diversity = len(unique_starters) / max(len(questions), 1)\n",
        "\n",
        "        return {\n",
        "            'unique_ratio': unique_ratio,\n",
        "            'avg_length': avg_length,\n",
        "            'type_diversity': type_diversity\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# ENTRENADOR PERSONALIZADO CON CURRICULUM LEARNING\n",
        "# ============================================================================\n",
        "\n",
        "class CurriculumTrainer(Trainer):\n",
        "    \"\"\"Entrenador personalizado con curriculum learning y métricas avanzadas\"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.metrics_calculator = QuestionGenerationMetrics()\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
        "        \"\"\"Función de pérdida personalizada con estabilización\"\"\"\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.get(\"loss\")\n",
        "\n",
        "        # Estabilización de la pérdida para evitar NaN\n",
        "        if loss is not None:\n",
        "            # Gradient clipping implícito\n",
        "            loss = torch.clamp(loss, max=10.0)\n",
        "\n",
        "            # Verificar NaN/Inf\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(\"Detectado NaN/Inf en loss, usando valor de backup\")\n",
        "                loss = torch.tensor(1.0, device=loss.device, requires_grad=True)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
        "        \"\"\"Crear optimizador y scheduler personalizados\"\"\"\n",
        "        # Optimizador AdamW con parámetros específicos para LoRA\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=self.args.learning_rate,\n",
        "            weight_decay=self.args.weight_decay,\n",
        "            betas=(0.9, 0.95),  # Betas optimizadas para modelos de lenguaje\n",
        "            eps=1e-8\n",
        "        )\n",
        "\n",
        "        # Scheduler con warmup y cosine annealing\n",
        "        scheduler = get_scheduler(\n",
        "            name=self.args.lr_scheduler_type,\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=int(num_training_steps * self.args.warmup_ratio),\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "\n",
        "        return optimizer, scheduler\n",
        "\n",
        "# ============================================================================\n",
        "# FUNCIONES DE GENERACIÓN MEJORADAS\n",
        "# ============================================================================\n",
        "\n",
        "def generate_advanced_question(context, answer, model, tokenizer, generation_config=None):\n",
        "    \"\"\"Genera preguntas con configuración avanzada y múltiples estrategias\"\"\"\n",
        "    if generation_config is None:\n",
        "        generation_config = GENERATION_CONFIG\n",
        "\n",
        "    try:\n",
        "        # Acortar contexto si es necesario\n",
        "        max_context_length = 400\n",
        "        if len(context) > max_context_length:\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', context)\n",
        "            truncated_context = \"\"\n",
        "            for sentence in sentences:\n",
        "                if len(truncated_context + sentence) < max_context_length:\n",
        "                    truncated_context += sentence + \" \"\n",
        "                else:\n",
        "                    break\n",
        "            context = truncated_context.strip() or context[:max_context_length]\n",
        "\n",
        "        # Múltiples formatos de prompt para diversidad\n",
        "        prompt_formats = [\n",
        "            f\"### Contexto: {context}\\n### Respuesta: {answer}\\n### Pregunta:\",\n",
        "            f\"Basándose en el siguiente contexto, genere una pregunta cuya respuesta sea '{answer}':\\n\\nContexto: {context}\\n\\nPregunta:\",\n",
        "            f\"Contexto: {context}\\n\\nDada la respuesta '{answer}', ¿cuál sería una pregunta apropiada?\\n\\nPregunta:\"\n",
        "        ]\n",
        "\n",
        "        prompt = random.choice(prompt_formats)\n",
        "\n",
        "        # Tokenizar\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=400)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Configuración de generación robusta\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=generation_config[\"max_new_tokens\"],\n",
        "                temperature=generation_config[\"temperature\"],\n",
        "                top_p=generation_config[\"top_p\"],\n",
        "                top_k=generation_config[\"top_k\"],\n",
        "                num_beams=generation_config[\"num_beams\"],\n",
        "                do_sample=generation_config[\"do_sample\"],\n",
        "                early_stopping=generation_config[\"early_stopping\"],\n",
        "                no_repeat_ngram_size=generation_config[\"no_repeat_ngram_size\"],\n",
        "                repetition_penalty=generation_config[\"repetition_penalty\"],\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decodificar y limpiar\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extraer solo la pregunta generada\n",
        "        if \"### Pregunta:\" in generated_text:\n",
        "            question = generated_text.split(\"### Pregunta:\")[-1].strip()\n",
        "        elif \"Pregunta:\" in generated_text:\n",
        "            question = generated_text.split(\"Pregunta:\")[-1].strip()\n",
        "        else:\n",
        "            # Si no encontramos el marcador, tomar la parte final\n",
        "            lines = generated_text.split('\\n')\n",
        "            question = lines[-1].strip() if lines else \"\"\n",
        "\n",
        "        # Limpiar y validar la pregunta\n",
        "        question = question.split('\\n')[0].strip()  # Solo la primera línea\n",
        "        question = re.sub(r'^[^\\w¿]*', '', question)  # Limpiar inicio\n",
        "\n",
        "        # Asegurar que termine con signo de interrogación\n",
        "        if question and not question.endswith('?'):\n",
        "            question += '?'\n",
        "\n",
        "        # Validar calidad mínima\n",
        "        if len(question) < 10 or not any(word in question.lower() for word in ['qué', 'cuál', 'cómo', 'por qué', 'quién', 'dónde', 'cuándo']):\n",
        "            # Generar pregunta de fallback más sofisticada\n",
        "            question_templates = [\n",
        "                f\"¿Qué concepto se relaciona con {answer}?\",\n",
        "                f\"¿Cuál es la importancia de {answer} en este contexto?\",\n",
        "                f\"¿Cómo se define {answer} según el texto?\",\n",
        "                f\"¿Por qué es relevante {answer}?\",\n",
        "                f\"¿Qué papel desempeña {answer}?\"\n",
        "            ]\n",
        "            question = random.choice(question_templates)\n",
        "\n",
        "        return question\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en generación avanzada: {e}\")\n",
        "        # Fallback más sofisticado\n",
        "        fallback_questions = [\n",
        "            f\"¿Cuál es el concepto clave relacionado con {answer}?\",\n",
        "            f\"¿Qué información importante se presenta sobre {answer}?\",\n",
        "            f\"¿Cómo se explica {answer} en el contexto dado?\",\n",
        "            f\"¿Por qué es significativo {answer}?\",\n",
        "            f\"¿Qué aspectos destacan sobre {answer}?\"\n",
        "        ]\n",
        "        return random.choice(fallback_questions)\n",
        "\n",
        "def generate_sophisticated_distractors(context, correct_answer, question, model, tokenizer, num_distractors=3):\n",
        "    \"\"\"Genera distractores más sofisticados y plausibles\"\"\"\n",
        "    try:\n",
        "        # Prompt especializado para generar distractors\n",
        "        distractor_prompt = f\"\"\"### Tarea: Generar opciones incorrectas pero plausibles para una pregunta de opción múltiple.\n",
        "\n",
        "### Contexto: {context[:300]}\n",
        "\n",
        "### Pregunta: {question}\n",
        "### Respuesta correcta: {correct_answer}\n",
        "\n",
        "### Instrucciones: Genera {num_distractors} opciones incorrectas que:\n",
        "1. Sean relacionadas al tema pero incorrectas\n",
        "2. Tengan longitud similar a la respuesta correcta\n",
        "3. No sean obviamente falsas\n",
        "4. Se distingan claramente de la respuesta correcta\n",
        "\n",
        "### Opciones incorrectas:\n",
        "1.\"\"\"\n",
        "\n",
        "        inputs = tokenizer(distractor_prompt, return_tensors=\"pt\", truncation=True, max_length=450)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generar distractores\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=120,\n",
        "                temperature=0.9,\n",
        "                top_p=0.8,\n",
        "                num_beams=3,\n",
        "                do_sample=True,\n",
        "                no_repeat_ngram_size=2,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extraer distractores del texto generado\n",
        "        if \"### Opciones incorrectas:\" in generated_text:\n",
        "            distractors_section = generated_text.split(\"### Opciones incorrectas:\")[-1].strip()\n",
        "        else:\n",
        "            distractors_section = generated_text.split(\"1.\")[-1] if \"1.\" in generated_text else \"\"\n",
        "\n",
        "        # Procesar distractores generados\n",
        "        distractors = []\n",
        "        lines = distractors_section.split('\\n')\n",
        "\n",
        "        for line in lines[:num_distractors * 2]:  # Procesar más líneas para tener opciones\n",
        "            line = line.strip()\n",
        "            # Limpiar numeración y marcadores\n",
        "            line = re.sub(r'^\\d+[\\.\\)\\-]\\s*', '', line)\n",
        "            line = re.sub(r'^[\\-\\*\\•]\\s*', '', line)\n",
        "\n",
        "            if (line and len(line) > 3 and line not in distractors and\n",
        "                line.lower() != correct_answer.lower() and\n",
        "                correct_answer.lower() not in line.lower()):\n",
        "                distractors.append(line)\n",
        "\n",
        "                if len(distractors) >= num_distractors:\n",
        "                    break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando distractores: {e}\")\n",
        "        distractors = []\n",
        "\n",
        "    # Si no tenemos suficientes distractores generados, usar estrategia de fallback\n",
        "    while len(distractors) < num_distractors:\n",
        "        fallback_distractors = generate_fallback_distractors(context, correct_answer, question)\n",
        "        for distractor in fallback_distractors:\n",
        "            if distractor not in distractors and len(distractors) < num_distractors:\n",
        "                distractors.append(distractor)\n",
        "\n",
        "    return distractors[:num_distractors]\n",
        "\n",
        "def generate_fallback_distractors(context, correct_answer, question):\n",
        "    \"\"\"Genera distractores de fallback usando técnicas heurísticas\"\"\"\n",
        "    distractors = []\n",
        "\n",
        "    # Extraer entidades del contexto que no sean la respuesta correcta\n",
        "    entities = re.findall(r'\\b[A-Z][a-záéíóúñA-ZÁÉÍÓÚÑ]+(?:\\s+[A-Z][a-záéíóúñA-ZÁÉÍÓÚÑ]+)*\\b', context)\n",
        "    entities = [e for e in entities if e.lower() != correct_answer.lower() and len(e) > 2]\n",
        "\n",
        "    # Añadir entidades como distractores\n",
        "    distractors.extend(entities[:2])\n",
        "\n",
        "    # Generar distractores basados en el tipo de pregunta\n",
        "    question_lower = question.lower()\n",
        "\n",
        "    if any(word in question_lower for word in ['cuándo', 'año', 'fecha']):\n",
        "        # Para preguntas temporales\n",
        "        years = re.findall(r'\\b\\d{4}\\b', context)\n",
        "        if years:\n",
        "            distractors.extend([y for y in years if y != correct_answer][:2])\n",
        "        else:\n",
        "            distractors.extend(['1950', '1980', '2000'])\n",
        "\n",
        "    elif any(word in question_lower for word in ['quién', 'autor', 'inventor']):\n",
        "        # Para preguntas sobre personas\n",
        "        distractors.extend(['Charles Darwin', 'Isaac Newton', 'Marie Curie'])\n",
        "\n",
        "    elif any(word in question_lower for word in ['dónde', 'lugar', 'país']):\n",
        "        # Para preguntas de lugar\n",
        "        distractors.extend(['Francia', 'Alemania', 'Estados Unidos'])\n",
        "\n",
        "    elif any(word in question_lower for word in ['qué', 'cuál', 'concepto']):\n",
        "        # Para preguntas conceptuales\n",
        "        words = context.split()\n",
        "        technical_terms = [w for w in words if len(w) > 6 and w.istitle()]\n",
        "        distractors.extend(technical_terms[:2])\n",
        "\n",
        "    # Distractores genéricos más sofisticados\n",
        "    generic_distractors = [\n",
        "        'Información no especificada en el contexto',\n",
        "        'Concepto no desarrollado en el texto',\n",
        "        'Dato no proporcionado en la fuente',\n",
        "        'Elemento no mencionado directamente',\n",
        "        'Aspecto no abordado en el material'\n",
        "    ]\n",
        "\n",
        "    # Combinar todos los distractores y filtrar\n",
        "    all_distractors = distractors + generic_distractors\n",
        "    final_distractors = []\n",
        "\n",
        "    for dist in all_distractors:\n",
        "        if (len(dist) > 3 and dist not in final_distractors and\n",
        "            dist.lower() != correct_answer.lower() and\n",
        "            correct_answer.lower() not in dist.lower()):\n",
        "            final_distractors.append(dist)\n",
        "\n",
        "    return final_distractors[:5]\n",
        "\n",
        "# ============================================================================\n",
        "# FUNCIÓN DE ENTRENAMIENTO PRINCIPAL\n",
        "# ============================================================================\n",
        "\n",
        "def train_advanced_model():\n",
        "    \"\"\"Función principal de entrenamiento con configuración avanzada\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"INICIANDO ENTRENAMIENTO AVANZADO DEL GENERADOR DE PREGUNTAS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Inicializar WandB\n",
        "    wandb.init(\n",
        "        project=\"advanced-question-generation\",\n",
        "        name=f\"gpt-neo-advanced-{int(time.time())}\",\n",
        "        config=TRAINING_CONFIG\n",
        "    )\n",
        "\n",
        "    # Cargar modelo y tokenizer\n",
        "    try:\n",
        "        model, tokenizer = load_model_with_lora(MODEL_NAME, DEVICE)\n",
        "    except Exception as e:\n",
        "        print(f\"Error con modelo principal, usando backup: {e}\")\n",
        "        model, tokenizer = load_model_with_lora(BACKUP_MODEL, DEVICE)\n",
        "\n",
        "    # Crear dataset avanzado\n",
        "    print(\"\\nCreando dataset avanzado...\")\n",
        "    dataset = AdvancedQuestionDataset(\n",
        "        file_path=\"/content/xquad.es.json\",\n",
        "        use_synthetic=False,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Dividir dataset\n",
        "    train_size = int(0.85 * len(dataset))  # 85% para entrenamiento\n",
        "    val_size = len(dataset) - train_size\n",
        "\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(SEED)\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset dividido: {len(train_dataset)} entrenamiento, {len(val_dataset)} validación\")\n",
        "\n",
        "    # Configurar argumentos de entrenamiento\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        num_train_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
        "        per_device_train_batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
        "        per_device_eval_batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
        "        gradient_accumulation_steps=TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n",
        "        learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n",
        "        weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n",
        "        warmup_ratio=TRAINING_CONFIG[\"warmup_ratio\"],\n",
        "        lr_scheduler_type=TRAINING_CONFIG[\"lr_scheduler_type\"],\n",
        "        logging_steps=TRAINING_CONFIG[\"logging_steps\"],\n",
        "        save_strategy=TRAINING_CONFIG[\"save_strategy\"],\n",
        "        eval_strategy=TRAINING_CONFIG[\"eval_strategy\"],\n",
        "        save_total_limit=TRAINING_CONFIG[\"save_total_limit\"],\n",
        "        load_best_model_at_end=TRAINING_CONFIG[\"load_best_model_at_end\"],\n",
        "        metric_for_best_model=TRAINING_CONFIG[\"metric_for_best_model\"],\n",
        "        greater_is_better=TRAINING_CONFIG[\"greater_is_better\"],\n",
        "        dataloader_num_workers=TRAINING_CONFIG[\"dataloader_num_workers\"],\n",
        "        fp16=TRAINING_CONFIG[\"fp16\"],\n",
        "        max_grad_norm=TRAINING_CONFIG[\"max_grad_norm\"],\n",
        "        report_to=\"wandb\",\n",
        "        run_name=f\"advanced-qa-generation-{int(time.time())}\",\n",
        "        logging_dir=LOGS_DIR,\n",
        "        remove_unused_columns=False,\n",
        "        dataloader_pin_memory=True,\n",
        "        gradient_checkpointing=True,  # Para ahorrar memoria\n",
        "        optim=\"adamw_torch\",\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,  # No masked language modeling para modelos causales\n",
        "        pad_to_multiple_of=8  # Optimización para GPU\n",
        "    )\n",
        "\n",
        "    # Crear entrenador personalizado\n",
        "    trainer = CurriculumTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    # Explicitly create and assign optimizer and scheduler\n",
        "    # This is added to ensure they are initialized before trainer.train()\n",
        "    num_training_steps = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs\n",
        "    optimizer, scheduler = trainer.create_optimizer_and_scheduler(num_training_steps)\n",
        "    trainer.optimizer = optimizer\n",
        "    trainer.lr_scheduler = scheduler\n",
        "    print(\"\\nOptimizer and scheduler explicitly created and assigned.\")\n",
        "\n",
        "    # Entrenar modelo\n",
        "    print(\"\\nIniciando entrenamiento...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Guardar modelo final\n",
        "    print(\"\\nGuardando modelo final...\")\n",
        "    trainer.save_model(MODEL_SAVE_DIR)\n",
        "    tokenizer.save_pretrained(MODEL_SAVE_DIR)\n",
        "\n",
        "    print(f\"Modelo guardado en: {MODEL_SAVE_DIR}\")\n",
        "\n",
        "    # Finalizar WandB\n",
        "    wandb.finish()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# ============================================================================\n",
        "# FUNCIÓN DE EVALUACIÓN Y PRUEBAS\n",
        "# ============================================================================\n",
        "\n",
        "def comprehensive_evaluation(model, tokenizer, num_examples=5):\n",
        "    \"\"\"Evaluación comprehensiva del modelo entrenado\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EVALUACIÓN COMPREHENSIVA DEL MODELO\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Ejemplos de prueba diversos y complejos\n",
        "    test_examples = [\n",
        "        {\n",
        "            \"context\": \"La teoría de la relatividad general de Einstein revolucionó nuestra comprensión del espacio-tiempo. Propuesta en 1915, esta teoría describe la gravedad no como una fuerza, sino como una curvatura del espacio-tiempo causada por la masa y la energía. Las predicciones de Einstein fueron confirmadas experimentalmente durante el eclipse solar de 1919, cuando se observó que la luz de las estrellas se curvaba alrededor del Sol, validando así su teoría.\",\n",
        "            \"answer\": \"curvatura del espacio-tiempo causada por la masa y la energía\"\n",
        "        },\n",
        "        {\n",
        "            \"context\": \"El proceso de fotosíntesis en las plantas ocurre en dos fases principales: las reacciones dependientes de la luz (fase lumínica) y las reacciones independientes de la luz (ciclo de Calvin). Durante la fase lumínica, que ocurre en los tilacoides, la clorofila absorbe energía luminosa y la convierte en energía química (ATP y NADPH), liberando oxígeno como subproducto. En la fase oscura, que tiene lugar en el estroma del cloroplasto, el CO₂ atmosférico se combina con compuestos orgánicos utilizando el ATP y NADPH producidos anteriormente para formar glucosa.\",\n",
        "            \"answer\": \"tilacoides\"\n",
        "        },\n",
        "        {\n",
        "            \"context\": \"La Revolución Industrial del siglo XVIII marcó una transformación fundamental en la historia humana. Comenzó en Gran Bretaña alrededor de 1760 y se caracterizó por el desarrollo de nuevas tecnologías como la máquina de vapor de James Watt, perfeccionada en 1769. Esta innovación permitió la mecanización masiva de la producción textil y revolucionó el transporte con la aparición del ferrocarril. Los cambios sociales fueron profundos: surgimiento de la clase obrera industrial, migración masiva del campo a las ciudades, y nuevas dinámicas de trabajo basadas en horarios fijos y división del trabajo.\",\n",
        "            \"answer\": \"James Watt\"\n",
        "        },\n",
        "        {\n",
        "            \"context\": \"Los ecosistemas de arrecifes de coral se encuentran entre los más diversos y productivos del planeta. Estos ecosistemas dependen de una relación simbiótica mutuamente beneficiosa entre los pólipos coralinos y las zooxantelas, algas microscópicas que viven dentro de los tejidos del coral. Las zooxantelas realizan fotosíntesis y proporcionan hasta el 90% de la energía que necesitan los corales, mientras que los corales les ofrecen protección y nutrientes. Esta simbiosis es extremadamente sensible a cambios en la temperatura del agua, y el estrés térmico puede causar el blanqueamiento coralino, un fenómeno donde los corales expulsan las zooxantelas.\",\n",
        "            \"answer\": \"zooxantelas\"\n",
        "        },\n",
        "        {\n",
        "            \"context\": \"La inteligencia artificial ha experimentado un desarrollo exponencial en las últimas décadas, especialmente con el avance del aprendizaje profundo (deep learning). Las redes neuronales convolucionales (CNN) han revolucionado el reconocimiento de imágenes, mientras que las redes neuronales recurrentes (RNN) y más recientemente los transformers han transformado el procesamiento de lenguaje natural. El modelo GPT (Generative Pre-trained Transformer) representa un hito importante en la generación de texto, utilizando mecanismos de atención para comprender y generar lenguaje humano con notable coherencia y creatividad.\",\n",
        "            \"answer\": \"mecanismos de atención\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Métricas para evaluación\n",
        "    metrics_calculator = QuestionGenerationMetrics()\n",
        "    all_generated_questions = []\n",
        "    all_results = []\n",
        "\n",
        "    print(\"\\nGenerando preguntas de opción múltiple...\")\n",
        "\n",
        "    for i, example in enumerate(test_examples[:num_examples]):\n",
        "        print(f\"\\n{'-'*60}\")\n",
        "        print(f\"EJEMPLO {i+1}/{num_examples}\")\n",
        "        print(f\"{'-'*60}\")\n",
        "\n",
        "        context = example[\"context\"]\n",
        "        correct_answer = example[\"answer\"]\n",
        "\n",
        "        print(f\"CONTEXTO:\\n{textwrap.fill(context, width=75)}\")\n",
        "        print(f\"\\nRESPUESTA ESPERADA: {correct_answer}\")\n",
        "\n",
        "        try:\n",
        "            # Generar pregunta\n",
        "            generated_question = generate_advanced_question(\n",
        "                context, correct_answer, model, tokenizer\n",
        "            )\n",
        "            all_generated_questions.append(generated_question)\n",
        "\n",
        "            print(f\"PREGUNTA GENERADA: {generated_question}\")\n",
        "\n",
        "            # Generar distractores\n",
        "            distractors = generate_sophisticated_distractors(\n",
        "                context, correct_answer, generated_question, model, tokenizer\n",
        "            )\n",
        "\n",
        "            # Crear opciones de respuesta\n",
        "            all_options = [correct_answer] + distractors\n",
        "            random.shuffle(all_options)\n",
        "            correct_index = all_options.index(correct_answer)\n",
        "\n",
        "            print(f\"\\nOPCIONES DE RESPUESTA:\")\n",
        "            for j, option in enumerate(all_options):\n",
        "                marker = \" ✓ CORRECTA\" if j == correct_index else \"\"\n",
        "                print(f\"{chr(65+j)}. {option}{marker}\")\n",
        "\n",
        "            # Calcular métricas de calidad\n",
        "            # Para esto necesitaríamos preguntas de referencia, usaremos métricas heurísticas\n",
        "            result = {\n",
        "                \"context\": context,\n",
        "                \"answer\": correct_answer,\n",
        "                \"generated_question\": generated_question,\n",
        "                \"options\": all_options,\n",
        "                \"correct_index\": correct_index,\n",
        "                \"question_length\": len(generated_question.split()),\n",
        "                \"has_question_word\": any(word in generated_question.lower() for word in ['qué', 'cuál', 'cómo', 'por qué', 'quién', 'dónde', 'cuándo']),\n",
        "                \"ends_with_question_mark\": generated_question.endswith('?')\n",
        "            }\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR al procesar ejemplo {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Calcular métricas globales\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"MÉTRICAS DE EVALUACIÓN\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    if all_generated_questions:\n",
        "        diversity_metrics = metrics_calculator.calculate_diversity_metrics(all_generated_questions)\n",
        "\n",
        "        # Métricas de calidad\n",
        "        valid_questions = sum(1 for r in all_results if r[\"has_question_word\"] and r[\"ends_with_question_mark\"])\n",
        "        quality_score = valid_questions / len(all_results) if all_results else 0\n",
        "\n",
        "        avg_length = np.mean([r[\"question_length\"] for r in all_results])\n",
        "\n",
        "        print(f\"📊 MÉTRICAS GENERALES:\")\n",
        "        print(f\"   • Preguntas generadas exitosamente: {len(all_generated_questions)}/{num_examples}\")\n",
        "        print(f\"   • Calidad sintáctica: {quality_score:.2%}\")\n",
        "        print(f\"   • Longitud promedio: {avg_length:.1f} palabras\")\n",
        "\n",
        "        print(f\"\\n📈 MÉTRICAS DE DIVERSIDAD:\")\n",
        "        print(f\"   • Ratio de unicidad: {diversity_metrics['unique_ratio']:.2%}\")\n",
        "        print(f\"   • Diversidad de tipos: {diversity_metrics['type_diversity']:.2%}\")\n",
        "        print(f\"   • Longitud promedio: {diversity_metrics['avg_length']:.1f} palabras\")\n",
        "\n",
        "        # Guardar resultados\n",
        "        results_file = os.path.join(OUTPUT_DIR, \"evaluation_results.json\")\n",
        "        with open(results_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                \"results\": all_results,\n",
        "                \"metrics\": {\n",
        "                    \"quality_score\": quality_score,\n",
        "                    \"average_length\": avg_length,\n",
        "                    \"diversity_metrics\": diversity_metrics\n",
        "                }\n",
        "            }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"\\n💾 Resultados guardados en: {results_file}\")\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No se pudieron generar preguntas para evaluación\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# ============================================================================\n",
        "# FUNCIÓN PRINCIPAL\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Función principal que ejecuta todo el pipeline mejorado\"\"\"\n",
        "\n",
        "    print(\"🚀 INICIANDO GENERADOR AVANZADO DE PREGUNTAS\")\n",
        "    print(\"Universidad Militar Nueva Granada - Deep Learning\")\n",
        "    print(\"Autores: Juan Quiroga y Marielby Paz\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        # Paso 1: Entrenar el modelo\n",
        "        print(\"\\n🔧 PASO 1: ENTRENAMIENTO AVANZADO\")\n",
        "        model, tokenizer = train_advanced_model()\n",
        "\n",
        "        # Limpiar memoria después del entrenamiento\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Paso 2: Evaluación comprehensiva\n",
        "        print(\"\\n📊 PASO 2: EVALUACIÓN COMPREHENSIVA\")\n",
        "        evaluation_results = comprehensive_evaluation(model, tokenizer, num_examples=5)\n",
        "\n",
        "        print(\"\\n✅ PROCESO COMPLETADO EXITOSAMENTE\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"📁 Archivos generados en: {OUTPUT_DIR}/\")\n",
        "        print(f\"🤖 Modelo guardado en: {MODEL_SAVE_DIR}/\")\n",
        "        print(\"🎯 El modelo está listo para generar preguntas de alta calidad!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ ERROR CRÍTICO: {e}\")\n",
        "        print(\"Revisar logs para más detalles\")\n",
        "        raise\n",
        "\n",
        "# ============================================================================\n",
        "# EJECUCIÓN\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "KPrV4haJe4ie",
        "outputId": "112544a7-820c-4c50-fd9e-a8ef86b2ce6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilizando device: cuda\n",
            "GPU disponible: Tesla T4\n",
            "Memoria GPU total: 15.83 GB\n",
            "🚀 INICIANDO GENERADOR AVANZADO DE PREGUNTAS\n",
            "Universidad Militar Nueva Granada - Deep Learning\n",
            "Autores: Juan Quiroga y Marielby Paz\n",
            "================================================================================\n",
            "\n",
            "🔧 PASO 1: ENTRENAMIENTO AVANZADO\n",
            "================================================================================\n",
            "INICIANDO ENTRENAMIENTO AVANZADO DEL GENERADOR DE PREGUNTAS\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gpt-neo-advanced-1748315750</strong> at: <a href='https://wandb.ai/est-juand-quiroga-universidad-militar-nueva-granada/advanced-question-generation/runs/opdvdsg1' target=\"_blank\">https://wandb.ai/est-juand-quiroga-universidad-militar-nueva-granada/advanced-question-generation/runs/opdvdsg1</a><br> View project at: <a href='https://wandb.ai/est-juand-quiroga-universidad-militar-nueva-granada/advanced-question-generation' target=\"_blank\">https://wandb.ai/est-juand-quiroga-universidad-militar-nueva-granada/advanced-question-generation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250527_031550-opdvdsg1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250527_031632-xtfa7owz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/est-juand-quiroga-universidad-militar-nueva-granada/advanced-question-generation/runs/xtfa7owz' target=\"_blank\">gpt-neo-advanced-1748315792</a></strong> to <a href='https://wandb.ai/est-juand-quiroga-universidad-militar-nueva-granada/advanced-question-generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/est-juand-quiroga-universidad-militar-nueva-granada/advanced-question-generation' target=\"_blank\">https://wandb.ai/est-juand-quiroga-universidad-militar-nueva-granada/advanced-question-generation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/est-juand-quiroga-universidad-militar-nueva-granada/advanced-question-generation/runs/xtfa7owz' target=\"_blank\">https://wandb.ai/est-juand-quiroga-universidad-militar-nueva-granada/advanced-question-generation/runs/xtfa7owz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando modelo EleutherAI/gpt-neo-1.3B con configuración LoRA...\n",
            "trainable params: 7,864,320 || all params: 1,323,440,128 || trainable%: 0.5942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creando dataset avanzado...\n",
            "Cargados 3570 ejemplos de XQuAD (incluyendo aumentación)\n",
            "Dataset dividido: 3034 entrenamiento, 536 validación\n",
            "\n",
            "Optimizer and scheduler explicitly created and assigned.\n",
            "\n",
            "Iniciando entrenamiento...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='553' max='1140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 553/1140 2:43:40 < 2:54:22, 0.06 it/s, Epoch 5.81/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>34.636900</td>\n",
              "      <td>1.311493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>18.753000</td>\n",
              "      <td>0.826440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>9.922000</td>\n",
              "      <td>0.417066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5.021700</td>\n",
              "      <td>0.303157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.897800</td>\n",
              "      <td>0.270661</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}